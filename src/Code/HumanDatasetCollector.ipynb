{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPSGvpXCxicUoGQp+VMPFUk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RS_26mwNRqR4","executionInfo":{"status":"ok","timestamp":1768040928590,"user_tz":-120,"elapsed":3128,"user":{"displayName":"tzurdo1","userId":"01860308900252889326"}},"outputId":"0e7b443e-2faa-4827-e7ca-841d8020241b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Loading WikiText-103 dataset...\n","Collected 5000 samples\n","Removed 2 duplicates\n","\n","Dataset statistics:\n","  total_samples: 4998\n","  avg_length: 730\n","  vocabulary_size: 38393\n","  lexical_diversity: 0.056\n","\n","Dataset splits:\n","  Train: 3998 samples\n","  Validation: 500 samples\n","  Test: 500 samples\n","\n","Saved to: /content/drive/MyDrive/FinalProject/human_baseline_data\n"]}],"source":["import datasets\n","import numpy as np\n","from collections import Counter\n","import re\n","import hashlib\n","import json\n","import os\n","\n","class HumanDatasetCollector:\n","    def __init__(self):\n","        pass\n","\n","    def collect_wikitext(self, max_samples: int = 5000) -> list:\n","        print(f\"Loading WikiText-103 dataset...\")\n","        dataset = datasets.load_dataset('wikitext', 'wikitext-103-v1')\n","        train_data = dataset['train']\n","\n","        texts = []\n","        for example in train_data:\n","            if len(texts) >= max_samples:\n","                break\n","\n","            text = example['text'].strip()\n","            if self._is_quality_text(text):\n","                texts.append(self._clean_text(text))\n","\n","        print(f\"Collected {len(texts)} samples\")\n","        return texts\n","\n","    def _is_quality_text(self, text: str) -> bool:\n","        if len(text) < 100 or len(text) > 5000:\n","            return False\n","\n","        words = text.split()\n","        if len(words) < 20:\n","            return False\n","\n","        sentences = [s for s in text.split('.') if s.strip()]\n","        if len(sentences) < 2:\n","            return False\n","\n","        avg_length = np.mean([len(s.split()) for s in sentences])\n","        if avg_length < 5 or avg_length > 50:\n","            return False\n","\n","        return True\n","\n","    def _clean_text(self, text: str) -> str:\n","        text = re.sub(r'\\n\\n+', '\\n\\n', text)\n","        text = re.sub(r'= .* =', '', text)\n","        text = re.sub(r'\\[\\[.*?\\]\\]', '', text)\n","        text = re.sub(r'{{.*?}}', '', text)\n","        return text.strip()\n","\n","    def remove_duplicates(self, texts: list) -> list:\n","        seen = set()\n","        unique = []\n","        for text in texts:\n","            text_hash = hashlib.md5(text.encode()).hexdigest()\n","            if text_hash not in seen:\n","                seen.add(text_hash)\n","                unique.append(text)\n","\n","        if len(texts) > len(unique):\n","            print(f\"Removed {len(texts) - len(unique)} duplicates\")\n","        return unique\n","\n","    def split_dataset(self, texts: list, train=0.8, val=0.1):\n","        np.random.shuffle(texts)\n","        n = len(texts)\n","        train_end = int(n * train)\n","        val_end = int(n * (train + val))\n","\n","        splits = {\n","            'train': texts[:train_end],\n","            'validation': texts[train_end:val_end],\n","            'test': texts[val_end:]\n","        }\n","\n","        print(f\"\\nDataset splits:\")\n","        print(f\"  Train: {len(splits['train'])} samples\")\n","        print(f\"  Validation: {len(splits['validation'])} samples\")\n","        print(f\"  Test: {len(splits['test'])} samples\")\n","\n","        return splits\n","\n","    def get_statistics(self, texts: list) -> dict:\n","        all_words = ' '.join(texts).split()\n","        lengths = [len(text) for text in texts]\n","\n","        return {\n","            'total_samples': len(texts),\n","            'avg_length': int(np.mean(lengths)),\n","            'vocabulary_size': len(set(all_words)),\n","            'lexical_diversity': round(len(set(all_words)) / len(all_words), 3)\n","        }\n","\n","    def save_dataset(self, texts: list, filepath: str):\n","        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n","        with open(filepath, 'w', encoding='utf-8') as f:\n","            for text in texts:\n","                f.write(json.dumps({\"text\": text, \"source\": \"wikitext\"}) + \"\\n\")\n","\n","\n","def main():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    base_path = \"/content/drive/MyDrive/FinalProject/human_baseline_data\"\n","\n","    collector = HumanDatasetCollector()\n","    texts = collector.collect_wikitext(max_samples=5000)\n","    texts = collector.remove_duplicates(texts)\n","\n","    stats = collector.get_statistics(texts)\n","    print(f\"\\nDataset statistics:\")\n","    for key, value in stats.items():\n","        print(f\"  {key}: {value}\")\n","\n","    splits = collector.split_dataset(texts)\n","\n","    for split_name, split_data in splits.items():\n","        filepath = os.path.join(base_path, f\"{split_name}.jsonl\")\n","        collector.save_dataset(split_data, filepath)\n","\n","    print(f\"\\nSaved to: {base_path}\")\n","    return splits\n","\n","\n","if __name__ == \"__main__\":\n","    dataset_splits = main()"]}]}