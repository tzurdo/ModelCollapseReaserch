{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNHOqQmyPwfA5QA8tE1crL4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5qEotihlPxoH","executionInfo":{"status":"ok","timestamp":1769145322552,"user_tz":-120,"elapsed":20173,"user":{"displayName":"tzurdo1","userId":"01860308900252889326"}},"outputId":"08c0a0d5-1329-477d-b752-418d5f2d7c59"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Loading datasets...\n","Loaded 3998 human samples\n","Loaded 3180 AI samples\n","\n","Mixed dataset created:\n","  Human samples: 3180\n","  AI samples: 3180\n","  Total: 6360\n","\n","Dataset splits:\n","  Train: 5088 samples\n","  Validation: 636 samples\n","  Test: 636 samples\n"]}],"source":["import json\n","import random\n","import os\n","\n","\n","class MixedDatasetCreator:\n","    def __init__(self):\n","        pass\n","\n","    def load_dataset(self, filepath: str) -> list:\n","        with open(filepath, 'r') as f:\n","            return [json.loads(line) for line in f]\n","\n","    def create_mixed_dataset(self, human_data: list, ai_data: list, total_samples: int) -> list:\n","        # Calculate 50/50 split\n","        n_human = total_samples // 2\n","        n_ai = total_samples // 2\n","\n","        # Sample from each dataset\n","        human_sample = random.sample(human_data, min(n_human, len(human_data)))\n","        ai_sample = random.sample(ai_data, min(n_ai, len(ai_data)))\n","\n","        # Combine and shuffle\n","        mixed = human_sample + ai_sample\n","        random.shuffle(mixed)\n","\n","        print(f\"Mixed dataset created:\")\n","        print(f\"  Human samples: {len(human_sample)}\")\n","        print(f\"  AI samples: {len(ai_sample)}\")\n","        print(f\"  Total: {len(mixed)}\")\n","\n","        return mixed\n","\n","    def split_dataset(self, texts: list, train=0.8, val=0.1):\n","        random.shuffle(texts)\n","        n = len(texts)\n","        train_end = int(n * train)\n","        val_end = int(n * (train + val))\n","\n","        splits = {\n","            'train': texts[:train_end],\n","            'validation': texts[train_end:val_end],\n","            'test': texts[val_end:]\n","        }\n","\n","        print(f\"\\nDataset splits:\")\n","        print(f\"  Train: {len(splits['train'])} samples\")\n","        print(f\"  Validation: {len(splits['validation'])} samples\")\n","        print(f\"  Test: {len(splits['test'])} samples\")\n","\n","        return splits\n","\n","    def save_dataset(self, data: list, filepath: str):\n","        os.makedirs(os.path.dirname(filepath), exist_ok=True)\n","        with open(filepath, 'w', encoding='utf-8') as f:\n","            for item in data:\n","                f.write(json.dumps(item) + \"\\n\")\n","\n","\n","def main():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    project_root = \"/content/drive/MyDrive/FinalProject\"\n","\n","    # Get AI model name\n","    ai_model_name = \"gpt2_medium\"  # Change this to match your AI model folder\n","\n","    # Load human and AI datasets\n","    human_train = f\"{project_root}/human_baseline_data/train.jsonl\"\n","    ai_train = f\"{project_root}/ai_generated_data/{ai_model_name}/train.jsonl\"\n","    mixed_data_path = f\"{project_root}/mixed_data/{ai_model_name}\"\n","\n","    creator = MixedDatasetCreator()\n","\n","    print(\"Loading datasets...\")\n","    human_data = creator.load_dataset(human_train)\n","    ai_data = creator.load_dataset(ai_train)\n","\n","    print(f\"Loaded {len(human_data)} human samples\")\n","    print(f\"Loaded {len(ai_data)} AI samples\\n\")\n","\n","    # Create mixed dataset (use the smaller dataset size Ã— 2 to ensure 50/50)\n","    total_samples = min(len(human_data), len(ai_data)) * 2\n","    mixed_data = creator.create_mixed_dataset(human_data, ai_data, total_samples)\n","\n","    # Split into train/val/test\n","    splits = creator.split_dataset(mixed_data)\n","\n","    # Save all splits\n","    for split_name, split_data in splits.items():\n","        filepath = os.path.join(mixed_data_path, f\"{split_name}.jsonl\")\n","        creator.save_dataset(split_data, filepath)\n","\n","    return splits\n","\n","\n","if __name__ == \"__main__\":\n","    dataset_splits = main()"]}]}