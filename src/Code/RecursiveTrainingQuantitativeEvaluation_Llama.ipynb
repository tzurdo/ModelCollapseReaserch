{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOIj45IBnMvDl9TYi7bFaS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"edi1egZ1ttIp"},"outputs":[],"source":["import json\n","import torch\n","import numpy as np\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","from huggingface_hub import login\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import os\n","\n","\n","class ModelEvaluator:\n","    def __init__(self, base_model_name: str, trained_model_path: str = None):\n","        # Login\n","        try:\n","            from google.colab import userdata\n","            HF_TOKEN = userdata.get('HF_TOKEN')\n","            login(token=HF_TOKEN)\n","        except:\n","            pass\n","\n","        print(f\"Loading model: {base_model_name}\")\n","        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        # Load base model\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            base_model_name,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            low_cpu_mem_usage=True\n","        )\n","\n","        # Load trained model if path provided\n","        if trained_model_path:\n","            print(f\"Loading trained model from: {trained_model_path}\")\n","            self.model = PeftModel.from_pretrained(\n","                base_model,\n","                trained_model_path,\n","                is_trainable=False\n","            )\n","        else:\n","            self.model = base_model\n","\n","        print(\"Model loaded\\n\")\n","\n","    def generate_text(self, prompt: str, max_length: int = 150) -> str:\n","        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n","\n","        with torch.no_grad():\n","            outputs = self.model.generate(\n","                **inputs,\n","                max_length=max_length,\n","                temperature=0.8,\n","                do_sample=True,\n","                top_p=0.9,\n","                pad_token_id=self.tokenizer.eos_token_id\n","            )\n","\n","        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    def generate_from_prompts(self, prompts: list) -> list:\n","        print(f\"Generating from {len(prompts)} prompts...\")\n","        generated = []\n","        for i, prompt in enumerate(prompts):\n","            text = self.generate_text(prompt)\n","            generated.append(text)\n","            if (i + 1) % 10 == 0:\n","                print(f\"  Generated {i + 1}/{len(prompts)}\")\n","        return generated\n","\n","    def calculate_lexical_diversity(self, texts: list) -> dict:\n","        all_tokens = []\n","        for text in texts:\n","            tokens = text.lower().split()\n","            all_tokens.extend(tokens)\n","\n","        types = len(set(all_tokens))\n","        tokens_count = len(all_tokens)\n","        ttr = types / tokens_count if tokens_count > 0 else 0\n","\n","        bigrams = []\n","        for text in texts:\n","            words = text.lower().split()\n","            bigrams.extend([f\"{words[i]}_{words[i+1]}\" for i in range(len(words)-1)])\n","\n","        unique_bigrams = len(set(bigrams))\n","        total_bigrams = len(bigrams)\n","\n","        return {\n","            'type_token_ratio': round(ttr, 4),\n","            'vocabulary_size': types,\n","            'total_tokens': tokens_count,\n","            'bigram_diversity': round(unique_bigrams / total_bigrams if total_bigrams > 0 else 0, 4)\n","        }\n","\n","    def calculate_perplexity(self, texts: list) -> float:\n","        total_loss = 0\n","        count = 0\n","\n","        for text in texts:\n","            inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n","            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n","\n","            with torch.no_grad():\n","                outputs = self.model(**inputs, labels=inputs[\"input_ids\"])\n","                total_loss += outputs.loss.item()\n","                count += 1\n","\n","        avg_loss = total_loss / count\n","        perplexity = np.exp(avg_loss)\n","        return round(perplexity, 2)\n","\n","\n","def load_test_prompts(filepath: str, n: int = 50) -> list:\n","    \"\"\"Load test prompts - default 50 for real evaluation\"\"\"\n","    with open(filepath, 'r') as f:\n","        data = [json.loads(line) for line in f]\n","\n","    prompts = []\n","    for item in data[:n*2]:  # Get extra in case some are too short\n","        words = item['text'].split()\n","        if len(words) >= 20:\n","            prompt = ' '.join(words[:15])\n","            prompts.append(prompt)\n","            if len(prompts) >= n:\n","                break\n","\n","    return prompts\n","\n","\n","def main():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    project_root = \"/content/drive/MyDrive/FinalProject\"\n","    BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n","\n","    # Load test prompts - 50 for real evaluation\n","    test_file = f\"{project_root}/human_baseline_data/test.jsonl\"\n","    prompts = load_test_prompts(test_file, n=50)\n","    print(f\"Loaded {len(prompts)} test prompts\\n\")\n","\n","    # Models to evaluate - INCLUDING RECURSIVE GENERATIONS\n","    models_config = {\n","        'Base (Untrained)': None,\n","        'Human-trained': f\"{project_root}/trained_models_v2/human_baseline_data_llama\",\n","        'AI Gen 1': f\"{project_root}/trained_models_v2/ai_generated_data_gpt2_medium_llama\",\n","        'AI Gen 2': f\"{project_root}/trained_models_v2/ai_gen2_llama\",\n","        'AI Gen 3': f\"{project_root}/trained_models_v2/ai_gen3_llama\",\n","        'Mixed': f\"{project_root}/trained_models_v2/mixed_data_gpt2_medium_llama\"\n","    }\n","\n","    results = {}\n","\n","    for model_name, model_path in models_config.items():\n","        print(\"=\"*60)\n","        print(f\"EVALUATING: {model_name}\")\n","        print(\"=\"*60)\n","\n","        try:\n","            # Check if model exists (for Gen 2 and Gen 3 which may not exist yet)\n","            if model_path and not os.path.exists(model_path):\n","                print(f\"⚠️  Model not found. Skipping {model_name}\")\n","                print(f\"   Train this model first!\\n\")\n","                continue\n","\n","            evaluator = ModelEvaluator(BASE_MODEL, model_path)\n","            generated = evaluator.generate_from_prompts(prompts)\n","\n","            print(\"\\nCalculating metrics...\")\n","\n","            lexical = evaluator.calculate_lexical_diversity(generated)\n","            perplexity = evaluator.calculate_perplexity(generated)\n","\n","            results[model_name] = {\n","                'lexical_diversity': lexical,\n","                'perplexity': perplexity,\n","                'num_prompts': len(prompts)\n","            }\n","\n","            print(f\"\\n{model_name} Results:\")\n","            print(f\"  Type-Token Ratio: {lexical['type_token_ratio']}\")\n","            print(f\"  Vocabulary Size: {lexical['vocabulary_size']}\")\n","            print(f\"  Bigram Diversity: {lexical['bigram_diversity']}\")\n","            print(f\"  Perplexity: {perplexity}\")\n","            print()\n","\n","            del evaluator\n","            torch.cuda.empty_cache()\n","\n","        except Exception as e:\n","            print(f\"Error: {e}\")\n","            import traceback\n","            traceback.print_exc()\n","            print(\"Skipping...\\n\")\n","            continue\n","\n","    # Summary\n","    print(\"\\n\" + \"=\"*60)\n","    print(f\"RECURSIVE TRAINING EVALUATION ({len(prompts)} prompts)\")\n","    print(\"=\"*60)\n","\n","    # Print as table\n","    print(f\"\\n{'Model':<20} {'TTR':<8} {'Vocab':<8} {'Bigram':<8} {'Perplexity':<12}\")\n","    print(\"-\" * 62)\n","\n","    for model_name, metrics in results.items():\n","        ttr = metrics['lexical_diversity']['type_token_ratio']\n","        vocab = metrics['lexical_diversity']['vocabulary_size']\n","        bigram = metrics['lexical_diversity']['bigram_diversity']\n","        ppl = metrics['perplexity']\n","        print(f\"{model_name:<20} {ttr:<8.4f} {vocab:<8} {bigram:<8.4f} {ppl:<12.2f}\")\n","\n","    # Create Recursive Degradation Visualization\n","    if len(results) >= 3:  # Only if we have multiple generations\n","        create_recursive_degradation_chart(results, project_root)\n","\n","    # Save\n","    results_path = f\"{project_root}/evaluation_results_recursive.json\"\n","    with open(results_path, 'w') as f:\n","        json.dump(results, f, indent=2)\n","\n","    print(f\"\\n✓ Results saved to: {results_path}\")\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"RECURSIVE EVALUATION COMPLETE\")\n","    print(\"=\"*60)\n","\n","    return results\n","\n","\n","def create_recursive_degradation_chart(results: dict, project_root: str):\n","    \"\"\"Create line chart showing degradation across generations\"\"\"\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(\"GENERATING RECURSIVE DEGRADATION CHARTS\")\n","    print(\"=\"*60)\n","\n","    # Extract generation data\n","    generations = []\n","    ttr_values = []\n","    vocab_values = []\n","    bigram_values = []\n","\n","    # Map models to generations\n","    gen_mapping = {\n","        'Human-trained': 0,\n","        'AI Gen 1': 1,\n","        'AI Gen 2': 2,\n","        'AI Gen 3': 3\n","    }\n","\n","    for model_name in ['Human-trained', 'AI Gen 1', 'AI Gen 2', 'AI Gen 3']:\n","        if model_name in results:\n","            generations.append(gen_mapping[model_name])\n","            ttr_values.append(results[model_name]['lexical_diversity']['type_token_ratio'])\n","            vocab_values.append(results[model_name]['lexical_diversity']['vocabulary_size'])\n","            bigram_values.append(results[model_name]['lexical_diversity']['bigram_diversity'])\n","\n","    # Create line chart\n","    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","\n","    metrics_data = [\n","        ('Type-Token Ratio', ttr_values, 'TTR'),\n","        ('Vocabulary Size', vocab_values, 'Vocab'),\n","        ('Bigram Diversity', bigram_values, 'Bigram')\n","    ]\n","\n","    for ax, (title, values, label) in zip(axes, metrics_data):\n","        ax.plot(generations, values, marker='o', linewidth=2, markersize=10, color='red', label='Degradation')\n","        ax.set_xlabel('Generation', fontsize=12, fontweight='bold')\n","        ax.set_ylabel(label, fontsize=12, fontweight='bold')\n","        ax.set_title(f'{title} Degradation\\nAcross Recursive Training', fontsize=13, fontweight='bold')\n","        ax.grid(True, alpha=0.3)\n","        ax.set_xticks(generations)\n","        ax.set_xticklabels([f'Gen {g}' if g > 0 else 'Human' for g in generations])\n","\n","        # Add value labels\n","        for gen, val in zip(generations, values):\n","            ax.annotate(f'{val:.3f}' if isinstance(val, float) else f'{val}',\n","                       xy=(gen, val), xytext=(0, 10),\n","                       textcoords='offset points',\n","                       ha='center', fontweight='bold')\n","\n","        # Calculate degradation percentage\n","        if len(values) >= 2:\n","            total_degradation = ((values[0] - values[-1]) / values[0]) * 100\n","            ax.text(0.5, 0.05, f'Total Degradation: {total_degradation:.1f}%',\n","                   transform=ax.transAxes, ha='center',\n","                   bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5),\n","                   fontsize=10, fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{project_root}/recursive_degradation_line_chart.png', dpi=300, bbox_inches='tight')\n","    print(\"✓ Saved: recursive_degradation_line_chart.png\")\n","    plt.show()\n","\n","    # Create bar chart comparison\n","    fig, ax = plt.subplots(figsize=(12, 6))\n","\n","    model_names = list(results.keys())\n","    ttr_all = [results[m]['lexical_diversity']['type_token_ratio'] for m in model_names]\n","\n","    colors = ['gray' if 'Base' in m else 'green' if 'Human' in m else 'orange' if 'Mixed' in m else 'red' for m in model_names]\n","\n","    bars = ax.bar(range(len(model_names)), ttr_all, color=colors, alpha=0.8)\n","    ax.set_xlabel('Model', fontsize=12, fontweight='bold')\n","    ax.set_ylabel('Type-Token Ratio', fontsize=12, fontweight='bold')\n","    ax.set_title('Recursive Model Collapse: TTR Degradation', fontsize=14, fontweight='bold')\n","    ax.set_xticks(range(len(model_names)))\n","    ax.set_xticklabels(model_names, rotation=45, ha='right')\n","    ax.grid(True, alpha=0.3, axis='y')\n","\n","    # Add value labels\n","    for bar, val in zip(bars, ttr_all):\n","        height = bar.get_height()\n","        ax.text(bar.get_x() + bar.get_width()/2., height,\n","               f'{val:.3f}',\n","               ha='center', va='bottom', fontweight='bold')\n","\n","    plt.tight_layout()\n","    plt.savefig(f'{project_root}/recursive_collapse_bar_chart.png', dpi=300, bbox_inches='tight')\n","    print(\"✓ Saved: recursive_collapse_bar_chart.png\")\n","    plt.show()\n","\n","\n","if __name__ == \"__main__\":\n","    results = main()"]}]}