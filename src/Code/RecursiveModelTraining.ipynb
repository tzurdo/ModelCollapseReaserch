{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNx3K2Lp1YjsTWRrrMQXDns"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8Ynj-uO_tjfs"},"outputs":[],"source":["# New notebook: GenerateNextGeneration.ipynb\n","\n","import json\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from peft import PeftModel\n","from huggingface_hub import login\n","\n","\n","class RecursiveDataGenerator:\n","    def __init__(self, base_model_name: str, trained_model_path: str, generation: int):\n","        try:\n","            from google.colab import userdata\n","            HF_TOKEN = userdata.get('HF_TOKEN')\n","            login(token=HF_TOKEN)\n","        except:\n","            pass\n","\n","        self.generation = generation\n","        print(f\"Loading Generation {generation} model...\")\n","\n","        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n","        if self.tokenizer.pad_token is None:\n","            self.tokenizer.pad_token = self.tokenizer.eos_token\n","\n","        base_model = AutoModelForCausalLM.from_pretrained(\n","            base_model_name,\n","            torch_dtype=torch.float16,\n","            device_map=\"auto\",\n","            low_cpu_mem_usage=True\n","        )\n","\n","        self.model = PeftModel.from_pretrained(\n","            base_model,\n","            trained_model_path,\n","            is_trainable=False\n","        )\n","        print(\"Model loaded\\n\")\n","\n","    def generate_from_prompts(self, prompts: list, max_length: int = 200) -> list:\n","        print(f\"Generating Gen {self.generation + 1} data from {len(prompts)} prompts...\")\n","        generated = []\n","\n","        for i, prompt in enumerate(prompts):\n","            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n","\n","            with torch.no_grad():\n","                outputs = self.model.generate(\n","                    **inputs,\n","                    max_length=max_length,\n","                    temperature=0.8,\n","                    do_sample=True,\n","                    top_p=0.9,\n","                    pad_token_id=self.tokenizer.eos_token_id\n","                )\n","\n","            text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n","            text = text[len(prompt):].strip()\n","\n","            if len(text) > 50:\n","                generated.append(text)\n","\n","            if (i + 1) % 50 == 0:\n","                print(f\"  Generated {i + 1}/{len(prompts)}\")\n","\n","        return generated\n","\n","    def save_generation_data(self, texts: list, output_dir: str):\n","        import os\n","        os.makedirs(output_dir, exist_ok=True)\n","\n","        # Split into train/val/test\n","        import random\n","        random.shuffle(texts)\n","        n = len(texts)\n","        train_end = int(n * 0.8)\n","        val_end = int(n * 0.9)\n","\n","        splits = {\n","            'train': texts[:train_end],\n","            'validation': texts[train_end:val_end],\n","            'test': texts[val_end:]\n","        }\n","\n","        for split_name, split_data in splits.items():\n","            filepath = os.path.join(output_dir, f\"{split_name}.jsonl\")\n","            with open(filepath, 'w') as f:\n","                for text in split_data:\n","                    f.write(json.dumps({\n","                        \"text\": text,\n","                        \"source\": f\"ai_generated_gen{self.generation + 1}\",\n","                        \"model\": \"llama\",\n","                        \"generation\": self.generation + 1\n","                    }) + \"\\n\")\n","\n","        print(f\"Saved Gen {self.generation + 1} data to {output_dir}\")\n","\n","\n","def main():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    project_root = \"/content/drive/MyDrive/FinalProject\"\n","    BASE_MODEL = \"meta-llama/Llama-3.2-1B\"\n","\n","    # ============================================\n","    # CHANGE THESE FOR EACH GENERATION\n","    # ============================================\n","    CURRENT_GEN = 1  # Currently at Gen 1, will create Gen 2\n","    TRAINED_MODEL = f\"{project_root}/trained_models_v2/ai_generated_data_gpt2_medium_llama\"\n","    OUTPUT_DIR = f\"{project_root}/ai_generated_data_gen2\"\n","\n","    # Load prompts from human baseline\n","    with open(f\"{project_root}/human_baseline_data/train.jsonl\", 'r') as f:\n","        human_data = [json.loads(line) for line in f]\n","\n","    prompts = []\n","    for item in human_data[:3000]:\n","        words = item['text'].split()\n","        if len(words) >= 20:\n","            prompt = ' '.join(words[:15])\n","            prompts.append(prompt)\n","\n","    print(f\"Loaded {len(prompts)} prompts\\n\")\n","\n","    # Generate next generation\n","    generator = RecursiveDataGenerator(BASE_MODEL, TRAINED_MODEL, CURRENT_GEN)\n","    generated = generator.generate_from_prompts(prompts)\n","    generator.save_generation_data(generated, OUTPUT_DIR)\n","\n","    print(f\"\\nâœ“ Generation {CURRENT_GEN + 1} data created!\")\n","    print(f\"Next: Train on {OUTPUT_DIR}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}