{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.830188679245283,
  "eval_steps": 500,
  "global_step": 900,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.031446540880503145,
      "grad_norm": 0.5720809102058411,
      "learning_rate": 3.6e-05,
      "loss": 2.3964,
      "step": 10
    },
    {
      "epoch": 0.06289308176100629,
      "grad_norm": 0.6774102449417114,
      "learning_rate": 7.6e-05,
      "loss": 2.4448,
      "step": 20
    },
    {
      "epoch": 0.09433962264150944,
      "grad_norm": 0.8664562702178955,
      "learning_rate": 0.000116,
      "loss": 2.3316,
      "step": 30
    },
    {
      "epoch": 0.12578616352201258,
      "grad_norm": 0.7819448709487915,
      "learning_rate": 0.00015600000000000002,
      "loss": 2.339,
      "step": 40
    },
    {
      "epoch": 0.15723270440251572,
      "grad_norm": 0.7877326011657715,
      "learning_rate": 0.000196,
      "loss": 2.221,
      "step": 50
    },
    {
      "epoch": 0.18867924528301888,
      "grad_norm": 0.6860934495925903,
      "learning_rate": 0.00019800884955752213,
      "loss": 2.2919,
      "step": 60
    },
    {
      "epoch": 0.22012578616352202,
      "grad_norm": 0.8865070343017578,
      "learning_rate": 0.00019579646017699117,
      "loss": 2.1948,
      "step": 70
    },
    {
      "epoch": 0.25157232704402516,
      "grad_norm": 0.6805633306503296,
      "learning_rate": 0.0001935840707964602,
      "loss": 2.1831,
      "step": 80
    },
    {
      "epoch": 0.2830188679245283,
      "grad_norm": 0.8148905038833618,
      "learning_rate": 0.0001913716814159292,
      "loss": 2.1746,
      "step": 90
    },
    {
      "epoch": 0.31446540880503143,
      "grad_norm": 0.7013998627662659,
      "learning_rate": 0.00018915929203539825,
      "loss": 2.2018,
      "step": 100
    },
    {
      "epoch": 0.34591194968553457,
      "grad_norm": 0.5955281257629395,
      "learning_rate": 0.00018694690265486728,
      "loss": 2.2033,
      "step": 110
    },
    {
      "epoch": 0.37735849056603776,
      "grad_norm": 0.7161253094673157,
      "learning_rate": 0.0001847345132743363,
      "loss": 2.1334,
      "step": 120
    },
    {
      "epoch": 0.4088050314465409,
      "grad_norm": 0.5873593688011169,
      "learning_rate": 0.00018252212389380533,
      "loss": 2.1637,
      "step": 130
    },
    {
      "epoch": 0.44025157232704404,
      "grad_norm": 0.8175415396690369,
      "learning_rate": 0.00018030973451327437,
      "loss": 2.1579,
      "step": 140
    },
    {
      "epoch": 0.4716981132075472,
      "grad_norm": 0.6112439632415771,
      "learning_rate": 0.00017809734513274337,
      "loss": 2.1288,
      "step": 150
    },
    {
      "epoch": 0.5031446540880503,
      "grad_norm": 0.5611220002174377,
      "learning_rate": 0.0001758849557522124,
      "loss": 2.1632,
      "step": 160
    },
    {
      "epoch": 0.5345911949685535,
      "grad_norm": 0.560440719127655,
      "learning_rate": 0.00017367256637168142,
      "loss": 2.1554,
      "step": 170
    },
    {
      "epoch": 0.5660377358490566,
      "grad_norm": 0.6599816083908081,
      "learning_rate": 0.00017146017699115046,
      "loss": 2.0932,
      "step": 180
    },
    {
      "epoch": 0.5974842767295597,
      "grad_norm": 0.8293720483779907,
      "learning_rate": 0.0001692477876106195,
      "loss": 2.175,
      "step": 190
    },
    {
      "epoch": 0.6289308176100629,
      "grad_norm": 0.6693500876426697,
      "learning_rate": 0.0001670353982300885,
      "loss": 2.1916,
      "step": 200
    },
    {
      "epoch": 0.660377358490566,
      "grad_norm": 0.5383675694465637,
      "learning_rate": 0.00016482300884955754,
      "loss": 2.1981,
      "step": 210
    },
    {
      "epoch": 0.6918238993710691,
      "grad_norm": 0.6071017980575562,
      "learning_rate": 0.00016261061946902655,
      "loss": 2.1648,
      "step": 220
    },
    {
      "epoch": 0.7232704402515723,
      "grad_norm": 0.631200909614563,
      "learning_rate": 0.00016039823008849558,
      "loss": 2.1579,
      "step": 230
    },
    {
      "epoch": 0.7547169811320755,
      "grad_norm": 0.5618430376052856,
      "learning_rate": 0.0001581858407079646,
      "loss": 2.1252,
      "step": 240
    },
    {
      "epoch": 0.7861635220125787,
      "grad_norm": 0.5276978015899658,
      "learning_rate": 0.00015597345132743363,
      "loss": 2.0835,
      "step": 250
    },
    {
      "epoch": 0.8176100628930818,
      "grad_norm": 0.5119362473487854,
      "learning_rate": 0.00015376106194690264,
      "loss": 2.1648,
      "step": 260
    },
    {
      "epoch": 0.8490566037735849,
      "grad_norm": 0.5472806692123413,
      "learning_rate": 0.00015154867256637167,
      "loss": 2.111,
      "step": 270
    },
    {
      "epoch": 0.8805031446540881,
      "grad_norm": 0.5506889224052429,
      "learning_rate": 0.0001493362831858407,
      "loss": 2.1521,
      "step": 280
    },
    {
      "epoch": 0.9119496855345912,
      "grad_norm": 0.6549732685089111,
      "learning_rate": 0.00014712389380530972,
      "loss": 2.1078,
      "step": 290
    },
    {
      "epoch": 0.9433962264150944,
      "grad_norm": 0.6661525368690491,
      "learning_rate": 0.00014491150442477876,
      "loss": 2.2297,
      "step": 300
    },
    {
      "epoch": 0.9748427672955975,
      "grad_norm": 0.6397067308425903,
      "learning_rate": 0.0001426991150442478,
      "loss": 2.1679,
      "step": 310
    },
    {
      "epoch": 1.0062893081761006,
      "grad_norm": 0.5197979807853699,
      "learning_rate": 0.0001404867256637168,
      "loss": 2.1078,
      "step": 320
    },
    {
      "epoch": 1.0377358490566038,
      "grad_norm": 0.5437004566192627,
      "learning_rate": 0.00013827433628318584,
      "loss": 2.1319,
      "step": 330
    },
    {
      "epoch": 1.069182389937107,
      "grad_norm": 0.6682790517807007,
      "learning_rate": 0.00013606194690265488,
      "loss": 2.1261,
      "step": 340
    },
    {
      "epoch": 1.10062893081761,
      "grad_norm": 0.547270655632019,
      "learning_rate": 0.00013384955752212388,
      "loss": 2.0722,
      "step": 350
    },
    {
      "epoch": 1.1320754716981132,
      "grad_norm": 0.6601671576499939,
      "learning_rate": 0.00013163716814159292,
      "loss": 2.0297,
      "step": 360
    },
    {
      "epoch": 1.1635220125786163,
      "grad_norm": 0.5728386640548706,
      "learning_rate": 0.00012942477876106196,
      "loss": 2.068,
      "step": 370
    },
    {
      "epoch": 1.1949685534591195,
      "grad_norm": 0.5748276114463806,
      "learning_rate": 0.00012721238938053097,
      "loss": 2.0514,
      "step": 380
    },
    {
      "epoch": 1.2264150943396226,
      "grad_norm": 0.5511749386787415,
      "learning_rate": 0.000125,
      "loss": 2.1152,
      "step": 390
    },
    {
      "epoch": 1.2578616352201257,
      "grad_norm": 0.5597607493400574,
      "learning_rate": 0.00012278761061946904,
      "loss": 2.1476,
      "step": 400
    },
    {
      "epoch": 1.2893081761006289,
      "grad_norm": 0.6243411898612976,
      "learning_rate": 0.00012057522123893805,
      "loss": 2.1021,
      "step": 410
    },
    {
      "epoch": 1.320754716981132,
      "grad_norm": 0.6460784673690796,
      "learning_rate": 0.00011836283185840708,
      "loss": 2.0356,
      "step": 420
    },
    {
      "epoch": 1.3522012578616351,
      "grad_norm": 0.6722059845924377,
      "learning_rate": 0.00011615044247787612,
      "loss": 2.0314,
      "step": 430
    },
    {
      "epoch": 1.3836477987421385,
      "grad_norm": 0.6803529858589172,
      "learning_rate": 0.00011393805309734513,
      "loss": 2.1312,
      "step": 440
    },
    {
      "epoch": 1.4150943396226414,
      "grad_norm": 0.6274775862693787,
      "learning_rate": 0.00011172566371681417,
      "loss": 2.0782,
      "step": 450
    },
    {
      "epoch": 1.4465408805031448,
      "grad_norm": 0.6863551735877991,
      "learning_rate": 0.0001095132743362832,
      "loss": 2.0569,
      "step": 460
    },
    {
      "epoch": 1.4779874213836477,
      "grad_norm": 0.6687008738517761,
      "learning_rate": 0.00010730088495575221,
      "loss": 2.0533,
      "step": 470
    },
    {
      "epoch": 1.509433962264151,
      "grad_norm": 0.5905032157897949,
      "learning_rate": 0.00010508849557522125,
      "loss": 2.1072,
      "step": 480
    },
    {
      "epoch": 1.540880503144654,
      "grad_norm": 0.6917723417282104,
      "learning_rate": 0.00010287610619469028,
      "loss": 2.0817,
      "step": 490
    },
    {
      "epoch": 1.5723270440251573,
      "grad_norm": 0.6318933963775635,
      "learning_rate": 0.0001006637168141593,
      "loss": 2.101,
      "step": 500
    },
    {
      "epoch": 1.6037735849056602,
      "grad_norm": 0.5866632461547852,
      "learning_rate": 9.845132743362833e-05,
      "loss": 2.1036,
      "step": 510
    },
    {
      "epoch": 1.6352201257861636,
      "grad_norm": 0.6947861313819885,
      "learning_rate": 9.623893805309735e-05,
      "loss": 2.0726,
      "step": 520
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 0.6563246846199036,
      "learning_rate": 9.402654867256638e-05,
      "loss": 2.0859,
      "step": 530
    },
    {
      "epoch": 1.6981132075471699,
      "grad_norm": 0.6478065252304077,
      "learning_rate": 9.181415929203541e-05,
      "loss": 2.146,
      "step": 540
    },
    {
      "epoch": 1.7295597484276728,
      "grad_norm": 0.6716103553771973,
      "learning_rate": 8.960176991150443e-05,
      "loss": 2.1407,
      "step": 550
    },
    {
      "epoch": 1.7610062893081762,
      "grad_norm": 0.5954643487930298,
      "learning_rate": 8.738938053097346e-05,
      "loss": 2.069,
      "step": 560
    },
    {
      "epoch": 1.7924528301886793,
      "grad_norm": 0.671137273311615,
      "learning_rate": 8.517699115044248e-05,
      "loss": 2.0069,
      "step": 570
    },
    {
      "epoch": 1.8238993710691824,
      "grad_norm": 0.6202862858772278,
      "learning_rate": 8.29646017699115e-05,
      "loss": 2.1177,
      "step": 580
    },
    {
      "epoch": 1.8553459119496856,
      "grad_norm": 0.6986855864524841,
      "learning_rate": 8.075221238938053e-05,
      "loss": 2.0883,
      "step": 590
    },
    {
      "epoch": 1.8867924528301887,
      "grad_norm": 0.6513662338256836,
      "learning_rate": 7.853982300884956e-05,
      "loss": 2.0581,
      "step": 600
    },
    {
      "epoch": 1.9182389937106918,
      "grad_norm": 0.6797661781311035,
      "learning_rate": 7.632743362831859e-05,
      "loss": 2.1401,
      "step": 610
    },
    {
      "epoch": 1.949685534591195,
      "grad_norm": 0.6596166491508484,
      "learning_rate": 7.411504424778761e-05,
      "loss": 2.0852,
      "step": 620
    },
    {
      "epoch": 1.9811320754716981,
      "grad_norm": 0.6794798970222473,
      "learning_rate": 7.190265486725663e-05,
      "loss": 2.073,
      "step": 630
    },
    {
      "epoch": 2.0125786163522013,
      "grad_norm": 0.6257455348968506,
      "learning_rate": 6.969026548672567e-05,
      "loss": 2.0794,
      "step": 640
    },
    {
      "epoch": 2.0440251572327046,
      "grad_norm": 0.5925442576408386,
      "learning_rate": 6.747787610619469e-05,
      "loss": 2.0258,
      "step": 650
    },
    {
      "epoch": 2.0754716981132075,
      "grad_norm": 0.6335793733596802,
      "learning_rate": 6.526548672566371e-05,
      "loss": 2.0363,
      "step": 660
    },
    {
      "epoch": 2.106918238993711,
      "grad_norm": 0.7648054957389832,
      "learning_rate": 6.305309734513275e-05,
      "loss": 1.9382,
      "step": 670
    },
    {
      "epoch": 2.138364779874214,
      "grad_norm": 0.6833879947662354,
      "learning_rate": 6.084070796460177e-05,
      "loss": 2.0191,
      "step": 680
    },
    {
      "epoch": 2.169811320754717,
      "grad_norm": 0.7646492719650269,
      "learning_rate": 5.8628318584070795e-05,
      "loss": 2.0248,
      "step": 690
    },
    {
      "epoch": 2.20125786163522,
      "grad_norm": 0.7221628427505493,
      "learning_rate": 5.641592920353983e-05,
      "loss": 2.0376,
      "step": 700
    },
    {
      "epoch": 2.2327044025157234,
      "grad_norm": 0.7741159796714783,
      "learning_rate": 5.4203539823008854e-05,
      "loss": 2.0806,
      "step": 710
    },
    {
      "epoch": 2.2641509433962264,
      "grad_norm": 0.7335579991340637,
      "learning_rate": 5.1991150442477876e-05,
      "loss": 2.0608,
      "step": 720
    },
    {
      "epoch": 2.2955974842767297,
      "grad_norm": 0.7631743550300598,
      "learning_rate": 4.9778761061946906e-05,
      "loss": 1.9938,
      "step": 730
    },
    {
      "epoch": 2.3270440251572326,
      "grad_norm": 0.7306327223777771,
      "learning_rate": 4.7566371681415936e-05,
      "loss": 2.0163,
      "step": 740
    },
    {
      "epoch": 2.358490566037736,
      "grad_norm": 0.8022611737251282,
      "learning_rate": 4.535398230088496e-05,
      "loss": 2.1261,
      "step": 750
    },
    {
      "epoch": 2.389937106918239,
      "grad_norm": 0.7527194023132324,
      "learning_rate": 4.314159292035399e-05,
      "loss": 2.0485,
      "step": 760
    },
    {
      "epoch": 2.4213836477987423,
      "grad_norm": 0.7316129803657532,
      "learning_rate": 4.092920353982301e-05,
      "loss": 2.0341,
      "step": 770
    },
    {
      "epoch": 2.452830188679245,
      "grad_norm": 0.6736001372337341,
      "learning_rate": 3.8716814159292034e-05,
      "loss": 2.0134,
      "step": 780
    },
    {
      "epoch": 2.4842767295597485,
      "grad_norm": 0.7646310925483704,
      "learning_rate": 3.650442477876106e-05,
      "loss": 2.0219,
      "step": 790
    },
    {
      "epoch": 2.5157232704402515,
      "grad_norm": 0.702852725982666,
      "learning_rate": 3.4292035398230086e-05,
      "loss": 2.0315,
      "step": 800
    },
    {
      "epoch": 2.547169811320755,
      "grad_norm": 0.7312343120574951,
      "learning_rate": 3.2079646017699115e-05,
      "loss": 2.0398,
      "step": 810
    },
    {
      "epoch": 2.5786163522012577,
      "grad_norm": 0.9232547879219055,
      "learning_rate": 2.9867256637168145e-05,
      "loss": 2.0402,
      "step": 820
    },
    {
      "epoch": 2.610062893081761,
      "grad_norm": 0.8156008720397949,
      "learning_rate": 2.7654867256637168e-05,
      "loss": 1.9687,
      "step": 830
    },
    {
      "epoch": 2.641509433962264,
      "grad_norm": 0.7570106387138367,
      "learning_rate": 2.5442477876106197e-05,
      "loss": 1.9645,
      "step": 840
    },
    {
      "epoch": 2.6729559748427674,
      "grad_norm": 0.7179034948348999,
      "learning_rate": 2.3230088495575223e-05,
      "loss": 2.0332,
      "step": 850
    },
    {
      "epoch": 2.7044025157232703,
      "grad_norm": 0.6798260807991028,
      "learning_rate": 2.101769911504425e-05,
      "loss": 1.9922,
      "step": 860
    },
    {
      "epoch": 2.7358490566037736,
      "grad_norm": 0.6937389969825745,
      "learning_rate": 1.8805309734513272e-05,
      "loss": 2.1197,
      "step": 870
    },
    {
      "epoch": 2.767295597484277,
      "grad_norm": 0.6898998022079468,
      "learning_rate": 1.6592920353982302e-05,
      "loss": 1.9569,
      "step": 880
    },
    {
      "epoch": 2.79874213836478,
      "grad_norm": 0.7091063857078552,
      "learning_rate": 1.4380530973451328e-05,
      "loss": 2.0456,
      "step": 890
    },
    {
      "epoch": 2.830188679245283,
      "grad_norm": 0.7642886638641357,
      "learning_rate": 1.2168141592920354e-05,
      "loss": 2.0086,
      "step": 900
    }
  ],
  "logging_steps": 10,
  "max_steps": 954,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 4.31996232794112e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
